{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ae8691-90b4-4f5d-ae26-653b07595dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "import h5py\n",
    "import hermes.quiver as qv\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "#from export.mm_snapshotter import mm_add_streaming_input_preprocessor\n",
    "from utils.s3 import open_file\n",
    "\n",
    "from export.mm_modules import concatenation_layer\n",
    "\n",
    "def scale_model(model, instances):\n",
    "    \"\"\"\n",
    "    Scale the model to the number of instances per GPU desired\n",
    "    at inference time\n",
    "    \"\"\"\n",
    "    # TODO: should quiver handle this under the hood?\n",
    "    try:\n",
    "        model.config.scale_instance_group(instances)\n",
    "    except ValueError:\n",
    "        model.config.add_instance_group(count=instances)\n",
    "\n",
    "from hermes.quiver import Platform\n",
    "from hermes.quiver.streaming import utils as streaming_utils\n",
    "\n",
    "#from utils.preprocessing import mm_BackgroundSnapshotter, mm_BatchWhitener\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "from ml4gw.transforms import SpectralDensity, Whiten\n",
    "from ml4gw.utils.slicing import unfold_windows\n",
    "import numpy as np\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class PsdEstimator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Module that takes a sample of data, splits it into\n",
    "    two unequal-length segments, calculates the PSD of\n",
    "    the first section, then returns this PSD along with\n",
    "    the second section.\n",
    "\n",
    "    Args:\n",
    "        length:\n",
    "            The length, in seconds, of timeseries data\n",
    "            to be returned for whitening. Note that the\n",
    "            length of time used for the PSD will then be\n",
    "            whatever remains along first part of the time\n",
    "            axis of the input.\n",
    "        sample_rate:\n",
    "            Rate at which input data has been sampled in Hz\n",
    "        fftlength:\n",
    "            Length of FFTs to use when computing the PSD\n",
    "        overlap:\n",
    "            Amount of overlap between FFT windows when\n",
    "            computing the PSD. Default value of `None`\n",
    "            uses `fftlength / 2`\n",
    "        average:\n",
    "            Method for aggregating spectra from FFT\n",
    "            windows, either `\"mean\"` or `\"median\"`\n",
    "        fast:\n",
    "            If `True`, use a slightly faster PSD algorithm\n",
    "            that is inaccurate for the lowest two frequency\n",
    "            bins. If you plan on highpassing later, this\n",
    "            should be fine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        length: float,\n",
    "        sample_rate: float,\n",
    "        fftlength: float,\n",
    "        window: Optional[torch.Tensor] = None,\n",
    "        overlap: Optional[float] = None,\n",
    "        average: str = \"median\",\n",
    "        fast: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.size = int(length * sample_rate)\n",
    "        self.spectral_density = SpectralDensity(\n",
    "            sample_rate, fftlength, overlap, average, window=window, fast=fast\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        splits = [X.size(-1) - self.size, self.size]\n",
    "        background, X = torch.split(X, splits, dim=-1)\n",
    "\n",
    "        # if we have 2 batch elements in our input data,\n",
    "        # it will be assumed that the 0th element is data\n",
    "        # being used to calculate the psd to whiten the\n",
    "        # 1st element. Used when we want to use raw background\n",
    "        # data to calculate the PSDs to whiten data with injected signals\n",
    "        if X.ndim == 3 and X.size(0) == 2:\n",
    "            # 0th background element is used to calculate PSDs\n",
    "            background = background[0]\n",
    "            # 1st element is the data to be whitened\n",
    "            X = X[1]\n",
    "\n",
    "        psds = self.spectral_density(background.double())\n",
    "        return X, psds\n",
    "\n",
    "class BackgroundSnapshotter(torch.nn.Module):\n",
    "    \"\"\"Update a kernel with a new piece of streaming data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        psd_length,\n",
    "        kernel_length,\n",
    "        fduration,\n",
    "        sample_rate,\n",
    "        inference_sampling_rate,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        state_length = kernel_length + fduration + psd_length\n",
    "        state_length -= 1 / inference_sampling_rate\n",
    "        self.state_size = int(state_length * sample_rate)\n",
    "\n",
    "    def forward(self, update: Tensor, snapshot: Tensor) -> Tuple[Tensor, ...]:\n",
    "        x = torch.cat([snapshot, update], axis=-1)\n",
    "        snapshot = x[:, :, -self.state_size :]\n",
    "        return x, snapshot\n",
    "\n",
    "class mm_BatchWhitener(torch.nn.Module):\n",
    "    \"\"\"Calculate the PSDs and whiten an entire batch of kernels at once\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        resample_rates: Sequence[float], \n",
    "        kernel_lengths: Sequence[float], \n",
    "        high_passes: Sequence[float], \n",
    "        low_passes: Sequence[float],\n",
    "        inference_sampling_rates: Sequence[float],\n",
    "        starting_offsets: Sequence[int],\n",
    "        num_ifos: int,\n",
    "        kernel_length: float,\n",
    "        sample_rate: float,\n",
    "        batch_size: int,\n",
    "        fduration: float,\n",
    "        fftlength: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.resample_rates = resample_rates\n",
    "        self.stride_sizes = [int(sample_rate / isr) for isr in inference_sampling_rates]\n",
    "        self.kernel_sizes = [int(kl * sample_rate) for kl in kernel_lengths]\n",
    "        self.num_timeseries = len(kernel_lengths)-1\n",
    "        self.num_ifos = num_ifos\n",
    "        # do foreground length calculation in units of samples,\n",
    "        # then convert back to length to guard for intification\n",
    "        self.starting_offsets = [int(kernel_length*sample_rate-so*min(self.stride_sizes)-self.kernel_sizes[i]) \n",
    "                                 for i, so in enumerate(starting_offsets)]\n",
    "        self.ending_offsets = [None if int(so*min(self.stride_sizes)) == 0 else -int(so*min(self.stride_sizes)) for so in starting_offsets]\n",
    "        stride_size = sample_rate / max(inference_sampling_rates)\n",
    "        self.kernel_size = int(kernel_length * sample_rate)\n",
    "        strides = (batch_size - 1) * stride_size\n",
    "        fsize = int(fduration * sample_rate)\n",
    "        size = strides + self.kernel_size + fsize\n",
    "        length = size / sample_rate\n",
    "        self.psd_estimator = PsdEstimator(\n",
    "            length,\n",
    "            sample_rate,\n",
    "            fftlength=fftlength,\n",
    "            overlap=None,\n",
    "            average=\"median\",\n",
    "            fast=highpass is not None,\n",
    "        )\n",
    "        self.whiteners = torch.nn.ModuleList([Whiten(fduration, sample_rate, highpass, lowpass) \n",
    "                                              for highpass, lowpass in zip(high_passes, low_passes)])\n",
    "        self.resamplers = torch.nn.ModuleList([T.Resample(sample_rate, rr) for rr in resample_rates])\n",
    "        self.resample_rate = [sample_rate//rr for rr in resample_rates]\n",
    "        self.fft_highpass = high_passes[-1]\n",
    "        self.fft_lowpass = low_passes[-1]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Get the number of channels so we know how to\n",
    "        # reshape `x` appropriately after unfolding to\n",
    "        # ensure we have (batch, channels, time) shape\n",
    "        out_x = tuple()\n",
    "        x, psd = self.psd_estimator(x)\n",
    "        print(f'psd = {psd.shape}')\n",
    "        for i in range(self.num_timeseries):\n",
    "            whitened = self.whiteners[i](x.double(), psd)\n",
    "            print(f'self.whiteners[i](x.double(), psd) = {whitened.shape}')\n",
    "            sliced_x = whitened[..., self.starting_offsets[i]:self.ending_offsets[i]]\n",
    "            print(f'sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = {sliced_x.shape}')\n",
    "            sliced_x = unfold_windows(sliced_x, self.kernel_sizes[i], self.stride_sizes[i])\n",
    "            print(f'unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = {sliced_x.shape}')\n",
    "            sliced_x = sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i])\n",
    "            print(f'sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = {sliced_x.shape}')\n",
    "            bs = sliced_x.shape[0]\n",
    "            sliced_x = sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2)\n",
    "            print(f'sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = {sliced_x.shape}')\n",
    "            sliced_x = self.resamplers[i](sliced_x)\n",
    "            print(f'self.resamplers[i](sliced_x) = {sliced_x.shape}')\n",
    "            sliced_x = sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i]))\n",
    "            print(f'sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = {sliced_x.shape}')\n",
    "            out_x = out_x + (sliced_x,)\n",
    "        \n",
    "        whitened = self.whiteners[-1](x.double(), psd)\n",
    "        sliced_x = whitened[..., self.starting_offsets[-1]:self.ending_offsets[-1]]\n",
    "        sliced_x = unfold_windows(sliced_x, self.kernel_sizes[-1], self.stride_sizes[-1])\n",
    "        sliced_x = sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[-1])\n",
    "        bs = sliced_x.shape[0]\n",
    "        sliced_x = sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[-1])).squeeze(-2)\n",
    "        sliced_x = self.resamplers[-1](sliced_x)\n",
    "        sliced_x = sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[-1]//self.resample_rate[-1]))\n",
    "        freqs = torch.fft.rfftfreq(\n",
    "            sliced_x.shape[-1], d=1 / self.resample_rates[-1]\n",
    "        )\n",
    "        sliced_x = torch.fft.rfft(sliced_x)\n",
    "        mask = freqs >= self.fft_highpass\n",
    "        mask *= freqs <= self.fft_lowpass\n",
    "        sliced_x = sliced_x[:, :, mask]\n",
    "        freqs = np.linspace(0, self.resample_rates[-1]/2, psd.shape[-1])\n",
    "        mask = freqs >= self.fft_highpass\n",
    "        mask *= freqs <= self.fft_lowpass\n",
    "        asds = (psd[..., mask]**0.5 * 1e23).float()\n",
    "        asds = asds.unsqueeze(dim = 0)\n",
    "        asds = F.interpolate(asds, size=(sliced_x.shape[-1],), mode=\"linear\", align_corners=False)\n",
    "        asds = asds.repeat(sliced_x.shape[0], 1, 1)\n",
    "        sliced_x = torch.cat((sliced_x.real, sliced_x.imag, 1/asds), dim=1)\n",
    "        out_x = out_x + (sliced_x,)\n",
    "        return out_x\n",
    "\n",
    "def mm_add_streaming_input_preprocessor(\n",
    "    input_shapes: list,\n",
    "    ensemble: \"EnsembleModel\",\n",
    "    input: list,\n",
    "    psd_length: float,\n",
    "    sample_rate: float,\n",
    "    kernel_length: float,\n",
    "    inference_sampling_rate: float,\n",
    "    fduration: float,\n",
    "    fftlength: float,\n",
    "    resample_rates: Sequence[float], \n",
    "    kernel_lengths: Sequence[float], \n",
    "    high_passes: Sequence[float], \n",
    "    low_passes: Sequence[float],\n",
    "    inference_sampling_rates: Sequence[float],\n",
    "    starting_offsets: Sequence[int],\n",
    "    num_ifos: int,\n",
    "    q: Optional[float] = None,\n",
    "    highpass: Optional[float] = None,\n",
    "    lowpass: Optional[float] = None,\n",
    "    preproc_instances: Optional[int] = None,\n",
    "    streams_per_gpu: int = 1,\n",
    ") -> \"ExposedTensor\":\n",
    "    \"\"\"Create a snapshotter model and add it to the repository\"\"\"\n",
    "\n",
    "    augmentor = None\n",
    "\n",
    "    snapshotter = BackgroundSnapshotter(\n",
    "        psd_length=psd_length,\n",
    "        kernel_length=kernel_length,\n",
    "        fduration=fduration,\n",
    "        sample_rate=sample_rate,\n",
    "        inference_sampling_rate=inference_sampling_rate,\n",
    "    )\n",
    "\n",
    "    stride = int(sample_rate / inference_sampling_rate)\n",
    "    state_shape = (2, num_ifos, snapshotter.state_size)\n",
    "    input_shape = (2, num_ifos, batch_size * stride)\n",
    "    streaming_model = streaming_utils.add_streaming_model(\n",
    "        ensemble.repository,\n",
    "        streaming_layer=snapshotter,\n",
    "        name=\"snapshotter\",\n",
    "        input_name=\"stream\",\n",
    "        input_shape=input_shape,\n",
    "        state_names=[\"snapshot\"],\n",
    "        state_shapes=[state_shape],\n",
    "        output_names=[\"strain\"],\n",
    "        streams_per_gpu=streams_per_gpu,\n",
    "    )\n",
    "    ensemble.add_input(streaming_model.inputs[\"stream\"])\n",
    "    preprocessor = mm_BatchWhitener(\n",
    "        resample_rates = resample_rates, \n",
    "        kernel_lengths = kernel_lengths, \n",
    "        high_passes = high_passes, \n",
    "        low_passes = low_passes,\n",
    "        inference_sampling_rates = inference_sampling_rates,\n",
    "        starting_offsets = starting_offsets,\n",
    "        num_ifos = num_ifos,\n",
    "        kernel_length=kernel_length,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=batch_size,\n",
    "        fduration=fduration,\n",
    "        fftlength=fftlength,\n",
    "    )\n",
    "    preproc_model = ensemble.repository.add(\n",
    "        \"preprocessor\", platform=Platform.TORCHSCRIPT\n",
    "    )\n",
    "    # if we specified a number of instances we want per-gpu\n",
    "    # for each model at inference time, scale them now\n",
    "    if preproc_instances is not None:\n",
    "        scale_model(preproc_model, preproc_instances)\n",
    "\n",
    "    input_shape = streaming_model.outputs[\"strain\"].shape\n",
    "    preproc_model.export_version(\n",
    "        preprocessor,\n",
    "        input_shapes={\"strain\": input_shape},\n",
    "        output_names=[f\"whitened_{i}\" for i in range(len(input_shapes))],\n",
    "    )\n",
    "    ensemble.pipe(\n",
    "        streaming_model.outputs[\"strain\"],\n",
    "        preproc_model.inputs[\"strain\"],\n",
    "    )\n",
    "    return [preproc_model.outputs[f\"whitened_{i}\"] for i in range(len(input_shapes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c727a9-4759-4ce0-b9bb-745f9b629161",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "weights = '/home/seiya.tsukamoto/aframe/mm_v1/mm_v1/training/model.pt'\n",
    "batch_file = '/home/seiya.tsukamoto/aframe/mm_v1/mm_v1/training/batch.h5'\n",
    "repository_directory = '/home/seiya.tsukamoto/aframe/mm_v1/mm_v1/results/model_repo/'\n",
    "clean = False\n",
    "platform = qv.Platform.TENSORRT\n",
    "aframe_instances = None\n",
    "num_ifos = 2\n",
    "kernel_length = 2.375\n",
    "sample_rate = 2048\n",
    "classes = [64, 64, 64, 64]\n",
    "#layers = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
    "inference_sampling_rates = [8, 4, 2, 8]\n",
    "psd_length=8\n",
    "sample_rate=2048\n",
    "inference_sampling_rate=8\n",
    "fduration=1\n",
    "fftlength=None\n",
    "q = None\n",
    "highpass=1024\n",
    "lowpass=32\n",
    "preproc_instances=None\n",
    "streams_per_gpu=6\n",
    "resample_rates = [2048, 1024, 512, 2048]\n",
    "kernel_lengths = [0.5, 1, 2, 1]\n",
    "high_passes = [32, 32, 32, 32]\n",
    "low_passes = [1024, 128, 64, 1024]\n",
    "starting_offsets = [0, 1, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dbe2fc-2156-4d5c-90ab-238ddf43fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the model graph\n",
    "logging.info(\"Initializing model graph\")\n",
    "\n",
    "with open_file(weights, \"rb\") as f:\n",
    "    graph = torch.jit.load(f, map_location=\"cpu\")\n",
    "\n",
    "graph.eval()\n",
    "logging.info(f\"Initialize:\\n{graph}\")\n",
    "\n",
    "with open_file(batch_file, \"rb\") as f:\n",
    "    batch_file = h5py.File(io.BytesIO(f.read()))\n",
    "\n",
    "layers = sorted(batch_file.keys() - \"y\")\n",
    "input_shapes = [(batch_size*inference_sampling_rates[i]//max(inference_sampling_rates), \n",
    "                 batch_file[layer].shape[-2], \n",
    "                 batch_file[layer].shape[-1]) for i, layer in enumerate(layers)]\n",
    "n_layers = len(layers)\n",
    "\n",
    "graphs = []\n",
    "model_parent_dir = os.path.dirname(weights)\n",
    "for i in range(n_layers):\n",
    "    with open_file(os.path.join(model_parent_dir, f\"resnets_{i}.pt\"), \"rb\") as f:\n",
    "        graphs.append(torch.jit.load(f, map_location=\"cpu\"))\n",
    "        graphs[-1].eval()\n",
    "\n",
    "with open_file(os.path.join(model_parent_dir, f\"fc.pt\"), \"rb\") as f:\n",
    "    fc = torch.jit.load(f, map_location=\"cpu\")\n",
    "    fc.eval()\n",
    "# instantiate a model repository at the\n",
    "# indicated location. Split up the preprocessor\n",
    "# and the neural network (which we'll call aframe)\n",
    "# to export/scale them separately, and start by\n",
    "# seeing if either already exists in the model repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a404268f-8b7c-45fe-80a9-cb9aeddf7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f94542a7f10>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/20/2025-21:47:27] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f91a93cf2e0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f91a920bec0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f91b95563e0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f91b95426b0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fc/1/model.plan'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = qv.ModelRepository(repository_directory, clean)\n",
    "aframe = []\n",
    "for i in range(n_layers):\n",
    "    try:\n",
    "        aframe.append(repo.models[f\"resnet_{i}\"])\n",
    "    except KeyError:\n",
    "        aframe.append(repo.add(f\"resnet_{i}\", platform=platform))\n",
    "\n",
    "try:\n",
    "    aframe.append(repo.models[\"fc\"])\n",
    "except KeyError:\n",
    "    aframe.append(repo.add(\"fc\", platform=platform))\n",
    "\n",
    "try:\n",
    "    concatenation = repo.models[\"concatenation_layer\"]\n",
    "except KeyError:\n",
    "    concatenation = repo.add(\"concatenation_layer\", platform=platform)\n",
    "\n",
    "# if we specified a number of instances we want per-gpu\n",
    "# for each model at inference time, scale them now\n",
    "#if aframe_instances is not None:\n",
    "#    scale_model(aframe, aframe_instances)\n",
    "\n",
    "# the network will have some different keyword\n",
    "# arguments required for export depending on\n",
    "# the target inference platform\n",
    "# TODO: hardcoding these kwargs for now, but worth\n",
    "# thinking about a more robust way to handle this\n",
    "kwargs = {}\n",
    "if platform == qv.Platform.ONNX:\n",
    "    kwargs[\"opset_version\"] = 13\n",
    "\n",
    "    # turn off graph optimization because of this error\n",
    "    # https://github.com/triton-inference-server/server/issues/3418\n",
    "    aframe.config.optimization.graph.level = -1\n",
    "elif platform == qv.Platform.TENSORRT:\n",
    "    kwargs[\"use_fp16\"] = False\n",
    "\n",
    "for i in range(n_layers):\n",
    "    aframe[i].export_version(\n",
    "        graphs[i],\n",
    "        input_shapes={f\"whitened_{i}\": input_shapes[i]},\n",
    "        output_names=[f\"classes_{i}\"],\n",
    "        **kwargs,\n",
    "    )\n",
    "cl  = concatenation_layer(inference_sampling_rates)\n",
    "concatenation.export_version(\n",
    "    cl,\n",
    "    input_shapes={f\"classes_{i}\": (input_shapes[i][0], classes[i]) \n",
    "                  for i in range(n_layers)},\n",
    "    output_names=[\"concatenated\"],\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "aframe[-1].export_version(\n",
    "    fc,\n",
    "    input_shapes={\"concatenated\": (batch_size, sum(classes))},\n",
    "    output_names=[\"discriminator\"],\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a398cd57-6be1-47e8-81ab-3da93f0c8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_name = \"aframe-stream\"\n",
    "# if we don't, create one\n",
    "ensemble = repo.add(ensemble_name, platform=qv.Platform.ENSEMBLE)\n",
    "# if fftlength isn't specified, calculate the default value\n",
    "fftlength = fftlength or kernel_length + fduration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a757e8fb-d97b-496d-b749-c3b75b6c8bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/hermes/quiver/model.py:193: UserWarning: Triton expects specific naming conventions and ordering for tensor input names. Be careful. See https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#special-conventions-for-pytorch-backend\n",
      "  export_path = exporter(\n",
      "/usr/local/lib/python3.10/site-packages/hermes/quiver/exporters/exporter.py:244: UserWarning: Triton expects specific naming conventions and ordering for tensor output names. Be careful. See https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#special-conventions-for-pytorch-backend\n",
      "  output_shapes = self._get_output_shapes(model_fn, output_names)\n",
      "/tmp/ipykernel_1013857/4188429847.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if X.ndim == 3 and X.size(0) == 2:\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:42: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[-1] < nperseg:\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:523: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if N <= (2 * pad):\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:538: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if psd.size(-1) != num_freqs:\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:400: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  idx = int(highpass / df)\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:403: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  idx = int(lowpass / df)\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:406: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if inv_asd.size(-1) % 2:\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/spectral.py:424: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if 2 * pad < q.size(-1):\n",
      "/home/seiya.tsukamoto/.local/lib/python3.10/site-packages/ml4gw/utils/slicing.py:54: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if remainder == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psd = torch.Size([2, 3457])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 33536])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([128, 1, 2, 1024])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([128, 2, 1024])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([256, 1024])\n",
      "self.resamplers[i](sliced_x) = torch.Size([256, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([128, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 34560])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([64, 1, 2, 2048])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([64, 2, 2048])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([128, 2048])\n",
      "self.resamplers[i](sliced_x) = torch.Size([128, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([64, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 36608])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([32, 1, 2, 4096])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([32, 2, 4096])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([64, 4096])\n",
      "self.resamplers[i](sliced_x) = torch.Size([64, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([32, 2, 1024])\n",
      "psd = torch.Size([2, 3457])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 33536])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([128, 1, 2, 1024])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([128, 2, 1024])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([256, 1024])\n",
      "self.resamplers[i](sliced_x) = torch.Size([256, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([128, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 34560])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([64, 1, 2, 2048])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([64, 2, 2048])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([128, 2048])\n",
      "self.resamplers[i](sliced_x) = torch.Size([128, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([64, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 36608])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([32, 1, 2, 4096])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([32, 2, 4096])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([64, 4096])\n",
      "self.resamplers[i](sliced_x) = torch.Size([64, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([32, 2, 1024])\n",
      "psd = torch.Size([2, 3457])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 33536])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([128, 1, 2, 1024])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([128, 2, 1024])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([256, 1024])\n",
      "self.resamplers[i](sliced_x) = torch.Size([256, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([128, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 34560])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([64, 1, 2, 2048])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([64, 2, 2048])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([128, 2048])\n",
      "self.resamplers[i](sliced_x) = torch.Size([128, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([64, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 36608])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([32, 1, 2, 4096])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([32, 2, 4096])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([64, 4096])\n",
      "self.resamplers[i](sliced_x) = torch.Size([64, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([32, 2, 1024])\n",
      "psd = torch.Size([2, 3457])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 33536])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([128, 1, 2, 1024])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([128, 2, 1024])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([256, 1024])\n",
      "self.resamplers[i](sliced_x) = torch.Size([256, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([128, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 34560])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([64, 1, 2, 2048])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([64, 2, 2048])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([128, 2048])\n",
      "self.resamplers[i](sliced_x) = torch.Size([128, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([64, 2, 1024])\n",
      "self.whiteners[i](x.double(), psd) = torch.Size([1, 2, 37376])\n",
      "sliced_x[..., self.starting_offsets[i]:self.ending_offsets[i]] = torch.Size([1, 2, 36608])\n",
      "unfold_windows(whitened, self.kernel_sizes[i], self.stride_sizes[i]) = torch.Size([32, 1, 2, 4096])\n",
      "sliced_x.reshape(-1, self.num_ifos, self.kernel_sizes[i]) = torch.Size([32, 2, 4096])\n",
      "sliced_x.reshape((self.num_ifos*bs, 1, self.kernel_sizes[i])).squeeze(-2) = torch.Size([64, 4096])\n",
      "self.resamplers[i](sliced_x) = torch.Size([64, 1024])\n",
      "sliced_x.reshape((bs, self.num_ifos, self.kernel_sizes[i]//self.resample_rate[i])) = torch.Size([32, 2, 1024])\n"
     ]
    }
   ],
   "source": [
    "whitened = mm_add_streaming_input_preprocessor(\n",
    "    input_shapes = input_shapes,\n",
    "    ensemble = ensemble,\n",
    "    input = [aframe[i].inputs[f\"whitened_{i}\"] for i in range(n_layers)],\n",
    "    psd_length=psd_length,\n",
    "    sample_rate=sample_rate,\n",
    "    kernel_length=kernel_length,\n",
    "    inference_sampling_rate=inference_sampling_rate,\n",
    "    fduration=fduration,\n",
    "    fftlength=fftlength,\n",
    "    q=q,\n",
    "    highpass=highpass,\n",
    "    lowpass=lowpass,\n",
    "    preproc_instances=preproc_instances,\n",
    "    streams_per_gpu=streams_per_gpu,\n",
    "    resample_rates = resample_rates, \n",
    "    kernel_lengths = kernel_lengths, \n",
    "    high_passes = high_passes, \n",
    "    low_passes = low_passes,\n",
    "    inference_sampling_rates = inference_sampling_rates,\n",
    "    starting_offsets = starting_offsets,\n",
    "    num_ifos = num_ifos,\n",
    ")\n",
    "for i in range(n_layers):\n",
    "    ensemble.pipe(whitened[i], aframe[i].inputs[f\"whitened_{i}\"])\n",
    "\n",
    "for i in range(n_layers):\n",
    "    ensemble.pipe(aframe[i].outputs[f\"classes_{i}\"], concatenation.inputs[f\"classes_{i}\"])\n",
    "\n",
    "ensemble.pipe(concatenation.outputs[\"concatenated\"], aframe[-1].inputs[\"concatenated\"])\n",
    "ensemble.add_output(aframe[-1].outputs[\"discriminator\"])\n",
    "# export the ensemble model, which basically amounts\n",
    "# to writing its config and creating an empty version entry\n",
    "ensemble.export_version(None)\n",
    "\n",
    "# keep snapshot states around for a long time in case there are\n",
    "# unexpected bottlenecks which throttle update for a few seconds\n",
    "snapshotter = repo.models[\"snapshotter\"]\n",
    "snapshotter.config.sequence_batching.max_sequence_idle_microseconds = int(\n",
    "    6e10\n",
    ")\n",
    "snapshotter.config.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2bc5bd-3106-47c0-b640-213e9e689881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9013c-d4a7-4547-bee6-a3f5d81ec520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46646e-003c-466f-b228-2c6458f3e7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
