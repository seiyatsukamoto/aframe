# commented args represent values filled out
# by train task at run time. To build a functional
# standalone config, add these in.
 
# To start training from a checkpoint, uncomment the below argument
# and specify a path to the desired checkpoint
# ckpt_path: ""
model:
  class_path: train.model.multimodal.MultimodalSupervisedAframe
  init_args:
    arch:
      class_path: architectures.multimodal.MultiModalPsd
      init_args:
        num_ifos: 2
        low_freq_classes: 64
        high_freq_classes: 64
        freq_classes: 64
        low_freq_layers: [2, 2, 2]
        high_freq_layers: [2, 2, 2]
        freq_layers: [2, 2, 2]
        freq_kernel_size: 3
        zero_init_residual: true
        groups: 16
        width_per_group: 64
        norm_layer:
          class_path: ml4gw.nn.norm.GroupNorm1DGetter
          init_args:
            groups: 16
    metric:
      class_path: train.metrics.TimeSlideAUROC
      init_args:
        max_fpr: 1e-3
        pool_length: 8

    weight_decay: 0.0
    learning_rate: 0.000585
    pct_lr_ramp: 0.115
data:
  class_path: train.data.supervised.MultiModalSupervisedAframeDataset
  init_args:
    batch_size: 384
    batches_per_epoch: 3700
    num_files_per_batch: 10
    chunk_size: 10000
    chunks_per_epoch: 10
    psd_length: 8
    fftlength: null

    waveform_prob: 0.277
    swap_prob: 0.014
    mute_prob: 0.055
    left_pad: 0.25
    right_pad: 0.05
    snr_sampler:
      class_path: ml4gw.distributions.PowerLaw
      init_args: 
        minimum: 4
        maximum: 100
        index: -3
    valid_frac: 0.25
    valid_stride: 0.5
    num_valid_views: 5
    valid_livetime: 57600 
trainer:
  # by default, use a local CSV logger.
  # Options in train task for using a
  # wandb logger instead
  logger:
    - class_path: lightning.pytorch.loggers.CSVLogger
      init_args:
        # save_dir:
        flush_logs_every_n_steps: 10
  callbacks:
    # Uncomment below if you want to stop early based on validation score
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: "valid_auroc"
    #     mode: "max"
    #     patience:

    # custom model checkpoint for saving and
    # tracing best model at end of traiing
    # that will be used for downstream export
    - class_path: train.callbacks.ModelCheckpoint
      init_args:
        monitor: "valid_auroc"
        mode: "max"
        save_top_k: 1
        save_last: true
        auto_insert_metric_name: false
    - class_path: train.callbacks.SaveAugmentedBatch
  # uncomment below if you want to profile
  # profiler:
    # class_path: lightning.pytorch.profilers.PyTorchProfiler
    # dict_kwargs:
      # profile_memory: true
  # devices:
  # strategy: set to ddp if len(devices) > 1
  #precision: 16-mixed
  accelerator: auto
  max_epochs: 400
  check_val_every_n_epoch: 1
  log_every_n_steps: 20
  benchmark: true
